{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\py27\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_groups = 12\n",
    "    \n",
    "def parse_json(filepath, min_len=50, max_len=400):\n",
    "    with open(filepath) as fp:\n",
    "        reviews = json.load(fp)\n",
    "    \n",
    "    all_reviews = []\n",
    "    for hotel in reviews:\n",
    "        hotel_name = hotel['name']\n",
    "        hotel_reviews = hotel['comments']\n",
    "        \n",
    "        all_reviews += [review.lower() for review in hotel_reviews if min_len < len(review) < max_len]\n",
    "        \n",
    "    return all_reviews\n",
    "\n",
    "def parse_all_jsons(min_len=50, max_len=400):\n",
    "    all_reviews = []\n",
    "    \n",
    "    print \"Parsing jsons\"\n",
    "    with ThreadPoolExecutor(max_workers=n_groups) as executor: \n",
    "        futures = [executor.submit(parse_json, os.path.join('jsons', filename), min_len, max_len) for filename in os.listdir('jsons')]\n",
    "        for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "            all_reviews += future.result()\n",
    "      \n",
    "    return all_reviews\n",
    "        \n",
    "def train_w2v(min_len, max_len):\n",
    "    all_reviews = parse_all_jsons(min_len=min_len, max_len=max_len)\n",
    "    \n",
    "    print \"Training model\"\n",
    "    model = Word2Vec(all_reviews)\n",
    "    model.save('w2v.model')\n",
    "    \n",
    "def grouper(n, iterable, fillvalue=None):\n",
    "    args = [iter(iterable)] * n\n",
    "    return itertools.izip_longest(*args)\n",
    "\n",
    "def find_closer(text_min_len, text_max_len, sum_min_len, sum_max_len):\n",
    "    assert(os.path.isfile('w2v.model'))\n",
    "    \n",
    "    print \"Loading model\"\n",
    "    model = Word2Vec.load('w2v.model')\n",
    "\n",
    "    all_texts = parse_all_jsons(min_len=text_min_len, max_len=text_max_len)\n",
    "    all_absts = parse_all_jsons(min_len=sum_min_len, max_len=sum_max_len)\n",
    "    \n",
    "    print len(all_texts)\n",
    "    print len(all_absts)\n",
    "    \n",
    "    print \"Vectorizing reviews\"\n",
    "    def vectorize(model, all_reviews):\n",
    "        vects = []\n",
    "        with ThreadPoolExecutor(max_workers=n_groups) as executor: \n",
    "            def vec_task(model, reviews):\n",
    "                return [np.mean([model[w] for w in sent if w in model], axis=0) for sent in reviews if sent is not None]\n",
    "\n",
    "            groups = grouper(100, all_reviews)            \n",
    "            futures = [executor.submit(vec_task, model, group) for group in groups]\n",
    "\n",
    "            for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "                vects += future.result()\n",
    "                \n",
    "        return vects\n",
    "    \n",
    "    texts_vects = vectorize(model, all_texts)\n",
    "    absts_vects = vectorize(model, all_absts)\n",
    "    \n",
    "    print \"Finding data pairs\"\n",
    "    all_results = []\n",
    "    with ThreadPoolExecutor(max_workers=n_groups) as executor:        \n",
    "        def nn_task(offset, all_texts, all_absts, vects, absts_vects):\n",
    "            nn = NearestNeighbors(n_neighbors=2).fit(vects)\n",
    "            results = []\n",
    "            \n",
    "            for i, vect in enumerate(absts_vects):\n",
    "                if vect is None:\n",
    "                    continue\n",
    "                    \n",
    "                distances, indices = nn.kneighbors([vect], n_neighbors=5)\n",
    "                most_similar = [all_texts[offset + indices[0][k]] for k, d in enumerate(distances[0]) if d > 0]\n",
    "                rnd_idx = np.random.choice(range(len(most_similar)), size=min(len(most_similar), 2), replace=False)\n",
    "                    \n",
    "                for k in rnd_idx:\n",
    "                    summary = all_absts[i]\n",
    "                    text = most_similar[k]\n",
    "                    results += [pd.Series({'text': text, 'summary': summary})]\n",
    "                    \n",
    "            return results\n",
    "        \n",
    "        groups = grouper(100, texts_vects)\n",
    "        futures = [executor.submit(nn_task, k*100, all_texts, all_absts, group, absts_vects) for k, group in enumerate(groups)]\n",
    "        \n",
    "        for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "            all_results += future.result()\n",
    "\n",
    "    def normalize(t):\n",
    "        t = t.encode('ascii', 'ignore')\n",
    "        t = t.replace(\"\\t\", \" \")\n",
    "        t = t.replace(\"\\n\", \" \")\n",
    "        t = t.replace(\"=\", \"\")\n",
    "        return t\n",
    "            \n",
    "    return pd.DataFrame(all_results).applymap(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tostring(row):\n",
    "    text = '<s>' + '</s><s>'.join(sent_tokenize(row['text'])) + '</s>'\n",
    "    summary = '<s>' + '</s><s>'.join(sent_tokenize(row['summary'])) + '</s>'\n",
    "    return 'article=' + text + '\\tabstract=' + summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-08 04:15:06,594 : INFO : loading Word2Vec object from w2v.model\n",
      "2017-06-08 04:15:06,678 : INFO : loading wv recursively from w2v.model.wv.* with mmap=None\n",
      "2017-06-08 04:15:06,681 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-06-08 04:15:06,684 : INFO : setting ignored attribute cum_table to None\n",
      "2017-06-08 04:15:06,686 : INFO : loaded w2v.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing jsons\n",
      "\n",
      "Parsing jsons\n",
      "\n",
      "387680\n",
      "16231\n",
      "Vectorizing reviews\n",
      "\n",
      "\n",
      "Finding data pairs\n",
      "24/|/  1%|| 24/3877 [26:01<69:39:05, 65.08s/it]"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    #train_w2v(0, float('inf'))\n",
    "    df = find_closer(120, 250, 0, 120)\n",
    "    df.to_csv('data.csv')\n",
    "    \n",
    "    data = '\\n'.join(df.apply(tostring, axis=1))\n",
    "    with open('data.txt', 'w') as fp:\n",
    "        fp.write(data)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "df = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head(30).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "concat = [''.join(val) for val in df.values]\n",
    "voc = CountVectorizer().fit(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('vocab.bin', 'wb') as fp:\n",
    "    fp.writelines([k + ' ' + str(v) + '\\n' for k,v in voc.vocabulary_.iteritems()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = df.sample(frac=1)\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "\n",
    "data = '\\n'.join(df.apply(tostring, axis=1))\n",
    "with open('data.txt', 'w') as fp:\n",
    "    fp.write(data)\n",
    "\n",
    "data_train = '\\n'.join(train.apply(tostring, axis=1))\n",
    "with open('data_train.txt', 'w') as fp:\n",
    "    fp.write(data_train)\n",
    "    \n",
    "data_test = '\\n'.join(test.apply(tostring, axis=1))\n",
    "with open('data_test.txt', 'w') as fp:\n",
    "    fp.write(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
