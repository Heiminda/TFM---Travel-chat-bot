{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "import tensorflow as tf\n",
    "from batch_reader import Batcher\n",
    "from data import Vocab\n",
    "\n",
    "class TextSumarization(object):\n",
    "    def __init__(self, vocab, options=None):\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        self.options = {\n",
    "            'batch_size': 10,\n",
    "            'input_seq_length': 300,\n",
    "            'output_seq_length': 30,\n",
    "            'embedding_dim': 128,\n",
    "            'num_hidden': 256,\n",
    "            'encode_layers': 4,\n",
    "            'min_input_len': 5,\n",
    "            'lr': 0.15,\n",
    "            'min_lr': 0.01,\n",
    "            'max_grad_norm': 2,\n",
    "            'mode': 'train'\n",
    "        }\n",
    "        self.options.update(options if options else {})\n",
    "        self.options['input_vocab_size'] = len(self.vocab)\n",
    "        \n",
    "    def add_placeholders(self):\n",
    "        self.seq_input = tf.placeholder(tf.int32, [self.options['batch_size'], self.options['input_seq_length']], name='articles')\n",
    "        self.seq_output = tf.placeholder(tf.int32, [self.options['batch_size'], self.options['output_seq_length']], name='abstracts')\n",
    "        self.dropout_keep_prob_embedding = tf.placeholder(tf.float32, name=\"dropout_keep_prob_embedding\")\n",
    "\n",
    "        self.targets = tf.placeholder(tf.int32, [self.options['batch_size'], self.options['output_seq_length']], name='targets')\n",
    "        self.article_lens = tf.placeholder(tf.int32, [self.options['batch_size']], name='article_lens')\n",
    "        self.abstract_lens = tf.placeholder(tf.int32, [self.options['batch_size']], name='abstract_lens')\n",
    "        self.loss_weights = tf.placeholder(tf.float32, [self.options['batch_size'], self.options['output_seq_length']], name='loss_weights')\n",
    "\n",
    "    def add_ops(self):\n",
    "        encoder_inputs = tf.unstack(tf.transpose(self.seq_input))\n",
    "        decoder_inputs = tf.unstack(tf.transpose(self.seq_output))\n",
    "    \n",
    "        with tf.variable_scope(\"embedding\"), tf.device(\"/cpu:0\"):\n",
    "            embedding = tf.get_variable(\n",
    "                \"W\",\n",
    "                [self.options['input_vocab_size'], self.options['embedding_dim']],\n",
    "                initializer=tf.random_uniform_initializer(-1.0, 1.0))\n",
    "\n",
    "            embedded_encoder_drop = tf.nn.dropout(tf.nn.embedding_lookup(embedding, self.seq_input),\n",
    "                                                  self.dropout_keep_prob_embedding)\n",
    "            embedded_decoder_drop = tf.nn.embedding_lookup(embedding, self.seq_output)\n",
    "\n",
    "            encoder_input = tf.stack([embedded_encoder_drop[:, i, :] for i in range(self.options['input_seq_length'])], axis=1)\n",
    "            decoder_input = tf.stack([embedded_decoder_drop[:, i, :] for i in range(self.options['output_seq_length'])], axis=1)\n",
    "        #     encoder_input = [embedded_encoder_drop[:, i, :] for i in range(input_seq_length)]\n",
    "        #     decoder_input = [embedded_decoder_drop[:, i, :] for i in range(output_seq_length)]\n",
    "\n",
    "        for layer_i in range(self.options['encode_layers']):\n",
    "            with tf.variable_scope('encoder%d'%layer_i), tf.device('/gpu:0'):\n",
    "                cell_fw = tf.contrib.rnn.BasicLSTMCell(self.options['num_hidden'])\n",
    "                cell_bw = tf.contrib.rnn.BasicLSTMCell(self.options['num_hidden'])\n",
    "\n",
    "                (emb_encoder_inputs, emb_state) = tf.nn.bidirectional_dynamic_rnn(\n",
    "                    cell_fw,\n",
    "                    cell_bw,\n",
    "                    encoder_input,\n",
    "                    sequence_length=self.article_lens,\n",
    "                    dtype=tf.float32\n",
    "                )\n",
    "\n",
    "                encoder_input = tf.concat(emb_encoder_inputs, 2)\n",
    "\n",
    "        encoder_output = encoder_input\n",
    "        fw_state, bw_state = emb_state\n",
    "        dec_in_state = tf.reshape(tf.concat(fw_state, 1), (self.options['batch_size'], self.options['num_hidden']*2))\n",
    "\n",
    "        with tf.variable_scope(\"output_projection\"):\n",
    "            w = tf.get_variable(\n",
    "                'w', [self.options['num_hidden'], self.options['input_vocab_size']], dtype=tf.float32,\n",
    "                initializer=tf.truncated_normal_initializer(stddev=1e-4))\n",
    "\n",
    "            w_t = tf.transpose(w)\n",
    "            v = tf.get_variable(\n",
    "                'v', [self.options['input_vocab_size']], dtype=tf.float32,\n",
    "                initializer=tf.truncated_normal_initializer(stddev=1e-4))\n",
    "\n",
    "            #we use the cell memory state for information on sentence embedding\n",
    "        #     scores = tf.nn.xw_plus_b(encoder_state, W, b)\n",
    "        #     y = tf.nn.softmax(scores)\n",
    "        #     predictions = tf.argmax(scores, 1)\n",
    "\n",
    "        with tf.variable_scope('decoder'), tf.device('/gpu:0'):\n",
    "            cell = tf.contrib.rnn.BasicLSTMCell(self.options['num_hidden'])\n",
    "\n",
    "            encoder_output = [tf.reshape(x, [self.options['batch_size'], 1, 2*self.options['num_hidden']]) \n",
    "                               for x in tf.split(encoder_output, self.options['input_seq_length'], axis=1)]\n",
    "            enc_top_states = tf.concat(axis=1, values=encoder_output)\n",
    "\n",
    "            print(enc_top_states.shape)\n",
    "\n",
    "            attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "                num_units=256,\n",
    "                memory=enc_top_states)\n",
    "\n",
    "            cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                cell,\n",
    "                attention_mechanism,\n",
    "                attention_layer_size=256,\n",
    "                alignment_history=False)\n",
    "\n",
    "        #         tf.contrib.legacy_seq2seq.attention_decoder()\n",
    "\n",
    "            if self.options['mode'] == \"train\":\n",
    "                helper = tf.contrib.seq2seq.ScheduledEmbeddingTrainingHelper(\n",
    "                    inputs=decoder_input,\n",
    "                    sequence_length=self.abstract_lens,\n",
    "                    embedding=embedding,\n",
    "                    sampling_probability=0.9)\n",
    "\n",
    "                initial_state = cell.zero_state(dtype=tf.float32, batch_size=self.options['batch_size'])\n",
    "            elif self.options['mode'] == \"infer\":\n",
    "                helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "                    embedding=embedding,\n",
    "                    start_tokens=tf.tile([GO_SYMBOL], [batch_size]),\n",
    "                    end_token=END_SYMBOL)\n",
    "\n",
    "                initial_state = dec_in_state\n",
    "\n",
    "            decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                cell=cell,\n",
    "                helper=helper,\n",
    "                initial_state=initial_state)\n",
    "\n",
    "            final_outputs, decoder_state, exit_lens = tf.contrib.seq2seq.dynamic_decode(decoder)\n",
    "            decoder_outputs = final_outputs.rnn_output\n",
    "\n",
    "        # with tf.variable_scope('output'), tf.device('/gpu:0'):\n",
    "        #     model_outputs = []\n",
    "        #     for i in range(len(decoder_outputs)):\n",
    "        #         if i > 0:\n",
    "        #             tf.get_variable_scope().reuse_variables()\n",
    "        #             model_outputs.append(tf.nn.xw_plus_b(tf.cast(decoder_outputs[i], tf.float32), w, v))\n",
    "        with tf.variable_scope('output'), tf.device('/gpu:0'):\n",
    "            model_outputs = []\n",
    "            for i in range(self.options['batch_size']):\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "                model_outputs.append(tf.nn.xw_plus_b(decoder_outputs[:, exit_lens[i], :], w, v))\n",
    "            \n",
    "        with tf.variable_scope('loss'), tf.device('/gpu:0'):\n",
    "            if self.options['mode'] == 'train':\n",
    "                self._loss = tf.contrib.seq2seq.sequence_loss(decoder_outputs, self.targets, self.loss_weights)\n",
    "            else:\n",
    "                self._loss = tf.contrib.seq2seq.sequence_loss(model_outputs, self.targets, self.loss_weights)\n",
    "                \n",
    "            tf.summary.scalar('loss', tf.minimum(12.0, self._loss))\n",
    "            \n",
    "    def add_train(self):\n",
    "        \"\"\"Sets self._train_op, op to run for training.\"\"\"\n",
    "\n",
    "        self._lr_rate = tf.maximum(\n",
    "            self.options['min_lr'],  # min_lr_rate.\n",
    "            tf.train.exponential_decay(self.options['lr'], self.global_step, 30000, 0.98))\n",
    "\n",
    "        tvars = tf.trainable_variables()\n",
    "        with tf.device('/gpu:0'):\n",
    "            grads, global_norm = tf.clip_by_global_norm(tf.gradients(self._loss, tvars), self.options['max_grad_norm'])\n",
    "\n",
    "        tf.summary.scalar('global_norm', global_norm)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self._lr_rate)\n",
    "        tf.summary.scalar('learning_rate', self._lr_rate)\n",
    "        self._train_op = optimizer.apply_gradients(zip(grads, tvars), global_step=self.global_step, name='train_step')\n",
    "            \n",
    "    def build_graph(self):\n",
    "        tf.reset_default_graph()\n",
    "        self.add_placeholders()\n",
    "        self.add_ops()\n",
    "        self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        if self.options['mode'] == 'train':\n",
    "            self.add_train()\n",
    "        self._summaries = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(model, data_batcher, log_root='./log_root', train_dir='./log_root/train', checkpoint_secs=60,\n",
    "           max_run_steps=10000000):\n",
    "    \"\"\"Runs model training.\"\"\"\n",
    "    with tf.device('/cpu:0'):\n",
    "        model.build_graph()\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        summary_writer = tf.summary.FileWriter(train_dir)\n",
    "        sv = tf.train.Supervisor(logdir=log_root,\n",
    "                                 is_chief=True,\n",
    "                                 saver=saver,\n",
    "                                 summary_op=None,\n",
    "                                 save_summaries_secs=60,\n",
    "                                 save_model_secs=checkpoint_secs,\n",
    "                                 global_step=model.global_step)\n",
    "        \n",
    "        sess = sv.prepare_or_wait_for_session(config=tf.ConfigProto(allow_soft_placement=True))\n",
    "        running_avg_loss = 0\n",
    "        step = 0\n",
    "        \n",
    "        while not sv.should_stop() and step < max_run_steps:\n",
    "            (article_batch, abstract_batch, targets, article_lens, abstract_lens,\n",
    "            loss_weights, _, _) = data_batcher.NextBatch()\n",
    "            (_, summaries, loss, train_step) = model.run_train_step(\n",
    "                sess, article_batch, abstract_batch, targets, article_lens,\n",
    "                abstract_lens, loss_weights)\n",
    "\n",
    "            summary_writer.add_summary(summaries, train_step)\n",
    "            running_avg_loss = _RunningAvgLoss(running_avg_loss, loss, summary_writer, train_step)\n",
    "            step += 1\n",
    "            \n",
    "            if step % 100 == 0:\n",
    "                summary_writer.flush()\n",
    "\n",
    "        sv.Stop()\n",
    "        return running_avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-35a639ad89b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextSumarization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vocab'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mbatcher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data_train.bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'article'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'abstract'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/TFM/data.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_file, max_size)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mvocab_f\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab_f\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mpieces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'vocab'"
     ]
    }
   ],
   "source": [
    "model = TextSumarization({'a': 1, 'b': 2})\n",
    "vocab = Vocab('vocab', 1000000)\n",
    "batcher = Batcher('data_train.bin', vocab, model.config, 'article', 'abstract', 200, 20)\n",
    "train(model, batcher)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
