{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_groups = 12\n",
    "    \n",
    "def parse_json(filepath, min_len=50, max_len=400):\n",
    "    with open(filepath) as fp:\n",
    "        reviews = json.load(fp)\n",
    "    \n",
    "    all_reviews = []\n",
    "    for hotel in reviews:\n",
    "        hotel_name = hotel['name']\n",
    "        hotel_reviews = hotel['comments']\n",
    "        \n",
    "        all_reviews += [review.lower() for review in hotel_reviews if min_len < len(review) < max_len]\n",
    "        \n",
    "    return all_reviews\n",
    "\n",
    "def parse_all_jsons(min_len=50, max_len=400):\n",
    "    all_reviews = []\n",
    "    \n",
    "    print \"Parsing jsons\"\n",
    "    with ThreadPoolExecutor(max_workers=n_groups) as executor: \n",
    "        futures = [executor.submit(parse_json, os.path.join('jsons', filename), min_len, max_len) for filename in os.listdir('jsons')]\n",
    "        for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "            all_reviews += future.result()\n",
    "      \n",
    "    return all_reviews\n",
    "        \n",
    "def train_w2v(min_len, max_len):\n",
    "    all_reviews = parse_all_jsons(min_len=min_len, max_len=max_len)\n",
    "    \n",
    "    print \"Training model\"\n",
    "    model = Word2Vec(all_reviews)\n",
    "    model.save('w2v.model')\n",
    "    \n",
    "def grouper(n, iterable, fillvalue=None):\n",
    "    args = [iter(iterable)] * n\n",
    "    return itertools.izip_longest(*args)\n",
    "\n",
    "def find_closer(text_min_len, text_max_len, sum_min_len, sum_max_len):\n",
    "    assert(os.path.isfile('w2v.model'))\n",
    "    \n",
    "    print \"Loading model\"\n",
    "    model = Word2Vec.load('w2v.model')\n",
    "\n",
    "    all_texts = parse_all_jsons(min_len=text_min_len, max_len=text_max_len)\n",
    "    all_absts = parse_all_jsons(min_len=sum_min_len, max_len=sum_max_len)\n",
    "    \n",
    "    print len(all_texts)\n",
    "    print len(all_absts)\n",
    "    \n",
    "    print \"Vectorizing reviews\"\n",
    "    def vectorize(model, all_reviews):\n",
    "        vects = []\n",
    "        with ThreadPoolExecutor(max_workers=n_groups) as executor: \n",
    "            def vec_task(model, reviews):\n",
    "                return [np.mean([model[w] for w in sent if w in model], axis=0) for sent in reviews if sent is not None]\n",
    "\n",
    "            groups = grouper(100, all_reviews)            \n",
    "            futures = [executor.submit(vec_task, model, group) for group in groups]\n",
    "\n",
    "            for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "                vects += future.result()\n",
    "                \n",
    "        return vects\n",
    "    \n",
    "    texts_vects = vectorize(model, all_texts)\n",
    "    absts_vects = vectorize(model, all_absts)\n",
    "    \n",
    "    print \"Finding data pairs\"\n",
    "    all_results = []\n",
    "    with ThreadPoolExecutor(max_workers=n_groups) as executor:        \n",
    "        def nn_task(offset, all_texts, all_absts, vects, absts_vects):\n",
    "            nn = NearestNeighbors(n_neighbors=2).fit(vects)\n",
    "            results = []\n",
    "            \n",
    "            for i, vect in enumerate(absts_vects):\n",
    "                if vect is None:\n",
    "                    continue\n",
    "                    \n",
    "                distances, indices = nn.kneighbors([vect], n_neighbors=5)\n",
    "                most_similar = [all_texts[offset + indices[0][k]] for k, d in enumerate(distances[0]) if d > 0]\n",
    "                rnd_idx = np.random.choice(range(len(most_similar)), size=min(len(most_similar), 2), replace=False)\n",
    "                    \n",
    "                for k in rnd_idx:\n",
    "                    summary = all_absts[i]\n",
    "                    text = most_similar[k]\n",
    "                    results += [pd.Series({'text': text, 'summary': summary})]\n",
    "                    \n",
    "            return results\n",
    "        \n",
    "        groups = grouper(100, texts_vects)\n",
    "        futures = [executor.submit(nn_task, k*100, all_texts, all_absts, group, absts_vects) for k, group in enumerate(groups)]\n",
    "        \n",
    "        for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "            all_results += future.result()\n",
    "\n",
    "    def normalize(t):\n",
    "        t = t.encode('ascii', 'ignore')\n",
    "        t = t.replace(\"\\t\", \" \")\n",
    "        t = t.replace(\"\\n\", \" \")\n",
    "        t = t.replace(\"=\", \"\")\n",
    "        return t\n",
    "            \n",
    "    return pd.DataFrame(all_results).applymap(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tostring(row):\n",
    "    text = '<s>' + '</s><s>'.join(sent_tokenize(row['text'])) + '</s>'\n",
    "    summary = '<s>' + '</s><s>'.join(sent_tokenize(row['summary'])) + '</s>'\n",
    "    return 'article=' + text + '\\tabstract=' + summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    #train_w2v(0, float('inf'))\n",
    "    df = find_closer(120, 250, 0, 120)\n",
    "    df.to_csv('data.csv')\n",
    "    \n",
    "    data = '\\n'.join(df.apply(tostring, axis=1))\n",
    "    with open('data.txt', 'w') as fp:\n",
    "        fp.write(data)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "df = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
